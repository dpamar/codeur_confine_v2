{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](brain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régularisation L2\n",
    "## Mise en situation\n",
    "\n",
    "Je souhaite améliorer le test sur mon réseau de neurones. J'ai l'impression qu'il y a de l'overfit durant l'entraînement.\n",
    "\n",
    "## Surajustement\n",
    "On a déja vu (jours 7 et 8) ce que peuvent être les problèmes de biais, de variance et de surajustement. On va voir comment ça se décline ici.\n",
    "\n",
    "### Définition\n",
    "Pour rappels:\n",
    "* Le biais d'un modèle est l'erreur obtenue en entraînement. On n'arrive pas à faire mieux qu'un certain pourcentage d'erreur : c'est notre biais.\n",
    "* La variance est la différence entre le score à l'entraînement et celui du test. Là, si on arrive à faire sensiblement mieux sur le training, c'est que ce que le modèle reconnaît quelque chose de plus efficace pour lui, mais pas forcément bon pour nous. Il _surajuste_ les données de training.\n",
    "\n",
    "### Bien identifier les problèmes d'overfit\n",
    "Dans le cadre de notre reconnaissance de chiffres, supposons qu'on trouve un modèle à 96% sur le training et 95% sur le test. Le \"1%\" d'écart est la variance du modèle. Dans ce cas précis, il n'y a probablement pas de gros problème d'overfit.\n",
    "\n",
    "Plus exactement, s'il y en a un, il n'est pas urgent de se pencher dessus. On a 4% de biais, et travailler dessus est bien plus intéressant. Arriver à le réduire de moitié (faire 98% donc) peut par voie de conséquence monter le test à 97% peut-être, alors que se focaliser sur la réduction de la variance et la réduire de moitié nous fera attendre 95.5% !\n",
    "\n",
    "A l'inverse, notre détecteur de chats faisait, supposons, 90% au training, et 70% au test. Alors oui il y a un sacré biais (10%) mais il y a surtout une grosse variance. Réduire le biais de moitié nous fera faire du 95% au training, et probablement +4% sur le test; réduire la variance de moitié nous fera faire du 80% au test !\n",
    "\n",
    "Pour éviter de se disperser, il vaut mieux essayer de régler les problèmes les uns après les autres. Et donc, si le biais pose d'avantage de souci, on se concentrera sur d'autres méthodes en priorité, et si c'est la variance on pourra faire un peu de régularisation, comme on va le voir tout de suite.\n",
    "\n",
    "### Régularisation\n",
    "On avait vu qu'on pouvait ajouter, à l'entrainement, un certain terme de régularisation, la somme des carrés des $\\theta$ - et forcer du coup ces valeurs à ne pas devenir trop grandes. Un surajustement s'accompagne souvent d'une grande amplitude dans les coefficients, pour pouvoir mieux coller au problème.\n",
    "\n",
    "Là, on va tout simplement ajouter la somme des carrés des W. C'est tout. Le terme en plus sera \n",
    "\n",
    "$\\frac{\\lambda}{2m}\\sum{W^2}$\n",
    "\n",
    "### Dérivée\n",
    "Le coût change, et donc sa dérivée aussi. Mais enfin la c'est pas énorme :\n",
    "* les gradients des poids _b_ ne sont pas concernés par le changement et leur dérivée ne change donc pas\n",
    "* Pour les _W_, on ajoute simplement le terme $\\frac{\\lambda}{m}W$. Pour un paramètre donné, tous les autres supposés constants, la somme ne porte que sur des constantes (dérivée nulle donc) sauf pour le terme considéré, qui donne 2W.\n",
    "\n",
    "## Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'initialisation\n",
    "def uniform(in_dim, out_dim): return np.random.rand(out_dim, in_dim)\n",
    "def uniform_100(in_dim, out_dim): return uniform(in_dim, out_dim) * 0.01\n",
    "def gaussian(in_dim, out_dim):return np.random.randn(out_dim, in_dim)\n",
    "def xavier(in_dim, out_dim): return gaussian(in_dim, out_dim) / np.sqrt(in_dim)\n",
    "def he(in_dim, out_dim): return gaussian(in_dim, out_dim) * np.sqrt(2/in_dim)\n",
    "def bengio(in_dim, out_dim): return gaussian(in_dim, out_dim) / np.sqrt(out_dim + in_dim)\n",
    "\n",
    "init_functions = {'uniform'   : uniform,\n",
    "                 'uniform_100': uniform_100,\n",
    "                 'gaussian'   : gaussian,\n",
    "                 'xavier'     : xavier,\n",
    "                 'he'         : he,\n",
    "                 'bengio'     : bengio}\n",
    "\n",
    "# Les différentes fonctions\n",
    "def sigmoid(x) : return 1 / (1 + np.exp(-x))\n",
    "def tanh(x): return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "def relu(x): return np.maximum(x, 0)\n",
    "\n",
    "act_functions = {'sigmoid': sigmoid, 'tanh' : tanh, 'relu' : relu}\n",
    "\n",
    "# Leurs dérivées\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def d_tanh(x):\n",
    "    t = tanh(x)\n",
    "    return 1 - t**2\n",
    "\n",
    "def d_relu(x):\n",
    "    return x > 0\n",
    "\n",
    "act_derivates = {'sigmoid': d_sigmoid, 'tanh' : d_tanh, 'relu' : d_relu}\n",
    "\n",
    "# Passe en avant : 1 couche - on utilise le dictionnaire de fonctions\n",
    "def layer_forward_pass(X, W, b, activation):\n",
    "    Z = np.dot(W, X) + b\n",
    "    A = act_functions[activation](Z)\n",
    "    return Z, A\n",
    "\n",
    "# Passe en avant : toutes les couches\n",
    "def model_forward_pass(X, activations, parameters):\n",
    "    result = {}\n",
    "    result['A0'] = X\n",
    "    # Entrée de la première couche: X\n",
    "    A = X\n",
    "    for i in range(1, len(activations) + 1):\n",
    "        # Pour chaque couche, une passe en avant. Les W et b viennent de parameters\n",
    "        Z_next, A_next = layer_forward_pass(A, parameters['W' + str(i)], parameters['b' + str(i)], activations[i-1])\n",
    "        result['Z' + str(i)] = Z_next\n",
    "        result['A' + str(i)] = A_next\n",
    "        A = A_next\n",
    "    return result\n",
    "\n",
    "# Passe en arrière : 1 couche - on utilise le dictionnaire de dérivées\n",
    "def layer_backward_pass(dA, Z, A_prev, W, activation):\n",
    "    dZ = dA * act_derivates[activation](Z)\n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dW, db, dA_prev\n",
    "\n",
    "# Passe en arrière : toutes les couches\n",
    "def model_backward_pass(dA_last, parameters, forward_pass_results, activations):\n",
    "    gradients = {}\n",
    "    dA = dA_last\n",
    "    for i in range(len(activations), 0, -1):\n",
    "        dW, db, dA_prev = layer_backward_pass(dA,\n",
    "                                              forward_pass_results['Z' + str(i)],\n",
    "                                              forward_pass_results['A' + str(i-1)],\n",
    "                                              parameters['W' + str(i)],\n",
    "                                              activations[i-1])\n",
    "        gradients['dW' + str(i)] = dW\n",
    "        gradients['db' + str(i)] = db\n",
    "        dA = dA_prev\n",
    "    return gradients\n",
    "\n",
    "def train_model(X, Y, layer_dimensions, layer_activations, initializations,\n",
    "               learning_rate = 0.01, epochs = 10, batch_size = 64, lambd = 0,\n",
    "               show_cost = False):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    #Nombre de couches - hors celle des entrées\n",
    "    l = len(layer_dimensions)-1\n",
    "    \n",
    "    # Création de tous les paramètres\n",
    "    # A chaque étape, W a pour dimensions \"nb neurones de la couche\" x \"nb entrées\"\n",
    "    # Et b est un vecteur, une valeur par neurone\n",
    "    parameters = {}\n",
    "    for i in range(1, l+1):\n",
    "        parameters['W' + str(i)] = init_functions[initializations[i-1]](layer_dimensions[i-1], layer_dimensions[i])\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dimensions[i], 1))\n",
    "    \n",
    "    costs = []\n",
    "    # Apprentissage\n",
    "    for e in range(epochs):\n",
    "        for s in range(0, m, batch_size):\n",
    "            x_batch = X[:, s:s+batch_size]\n",
    "            y_batch = Y[:, s:s+batch_size]\n",
    "    \n",
    "            # Passe en avant\n",
    "            forward_pass_results = model_forward_pass(x_batch, layer_activations, parameters)\n",
    "            \n",
    "            # Calcul de la dérivée du coût par rapport au dernier A\n",
    "            A_last = forward_pass_results['A' + str(l)]\n",
    "            dA_last = -(np.divide(y_batch, A_last) - np.divide(1 - y_batch, 1 - A_last))/x_batch.shape[1]\n",
    "\n",
    "            # Calcul des gradients - passe en arrière\n",
    "            gradients = model_backward_pass(dA_last, parameters, forward_pass_results, layer_activations)\n",
    "            \n",
    "            # Descente de gradient\n",
    "            for i in range(1, l+1):\n",
    "                # Ajout de la régularisation L2\n",
    "                parameters['W' + str(i)] -= learning_rate * (gradients['dW' + str(i)] + lambd /x_batch.shape[1] * parameters['W' + str(i)])\n",
    "                parameters['b' + str(i)] -= learning_rate * gradients['db' + str(i)]\n",
    "        \n",
    "        # Un peu de debug\n",
    "        model_result = model_forward_pass(X, layer_activations, parameters)['A' + str(l)]\n",
    "        cost = np.squeeze(-np.sum(np.log(model_result) * Y + np.log(1 - model_result) * (1-Y))/m)\n",
    "        regularization_cost = lambd/2/m * np.sum([np.sum(parameters['W'+str(i)]**2) for i in range(1, l+1)])\n",
    "        cost += regularization_cost\n",
    "        costs.append(cost)\n",
    "        if show_cost :\n",
    "            if e % (epochs // 20) == 0: print('Epoch #%i: %s' % (e, cost))\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retour à la mise en situation\n",
    "\n",
    "### Un exemple de ce qu'il ne faut pas faire\n",
    "\n",
    "On a vu que traiter l'overfit sur notre modèle de traitement des chiffres était une mauvaise idée. On va le confirmer.\n",
    "\n",
    "C'est toujours le dataset de Yann Le Cun http://yann.lecun.com/exdb/mnist/ (images 28x28, 60.000 données d'entrainement et 10.000 données de validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file): \n",
    "    data = np.load(file)\n",
    "    return data['x'], data['y']\n",
    "\n",
    "x_train, y_train = load('data/d09_train_data.npz')\n",
    "x_test , y_test  = load('data/d09_test_data.npz')\n",
    "\n",
    "mus    = x_train.mean(axis = 0, keepdims = True)\n",
    "sigmas = x_train.std (axis = 0, keepdims = True) + 1e-9\n",
    "\n",
    "x_train_norm = (x_train-mus)/sigmas\n",
    "x_test_norm  = (x_test -mus)/sigmas\n",
    "\n",
    "y_train_mat = (y_train == np.arange(10)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allez, c'est parti. On va tester un réseau avec, et sans régularisation L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, params, layer_activations):\n",
    "    results = np.argmax(model_forward_pass(x, layer_activations, params)['A'+str(len(layer_activations))], axis = 0)\n",
    "    return np.mean(results == y)\n",
    "\n",
    "def test_reg(epochs, lambd = 0) :\n",
    "    np.random.seed(0)\n",
    "    activations = ['relu', 'relu', 'sigmoid']\n",
    "    params, costs = train_model(x_train_norm.T, y_train_mat.T, [28*28, 50, 25, 10], activations,\n",
    "                            ['xavier', 'xavier', 'xavier'], lambd = lambd,\n",
    "                            epochs = epochs, learning_rate = 0.005, show_cost = False)\n",
    "    print('Accuracy on training set : %f%%' % (100*accuracy(x_train_norm.T, y_train.T, params, activations)))\n",
    "    print('Accuracy on test set : %f%%'     % (100*accuracy(x_test_norm.T , y_test.T , params, activations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 96.906667%\n",
      "Accuracy on test set : 96.040000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8fbb3e7d7b5f>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "test_reg(epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 92.228333%\n",
      "Accuracy on test set : 92.280000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8fbb3e7d7b5f>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "test_reg(epochs = 20, lambd = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel est le résultat de notre bidouille? On a essayé de moins coller aux données d'entrainement, et résultat on fait moins bien. Et sur le test aussi :(\n",
    "\n",
    "Et si on persiste et qu'on continue sur cette voie, on peut toujours réduire le poids de la régularisation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 95.041667%\n",
      "Accuracy on test set : 94.570000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8fbb3e7d7b5f>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "test_reg(epochs = 20, lambd = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 96.746667%\n",
      "Accuracy on test set : 95.960000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8fbb3e7d7b5f>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "test_reg(epochs = 20, lambd = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ne fait au final que dégrader les performances en training, et avec le même impact sur le test. Un beau gachis.\n",
    "\n",
    "On va maintenant revenir aux chatons. On faisait 100% au training et 74% au test, donc un vrai souci là pour le coup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cat, y_train_cat = load('data/d20_train_data.npz')\n",
    "x_test_cat , y_test_cat  = load('data/d20_test_data.npz')\n",
    "\n",
    "mus    = x_train_cat.mean(axis = 1, keepdims = True)\n",
    "sigmas = x_train_cat.std (axis = 1, keepdims = True) + 1e-9\n",
    "\n",
    "x_train_cat_norm = (x_train_cat-mus)/sigmas\n",
    "x_test_cat_norm  = (x_test_cat -mus)/sigmas\n",
    "\n",
    "def accuracy_cat(x, y, params, layer_activations):\n",
    "    results = model_forward_pass(x, layer_activations, params)['A'+str(len(layer_activations))] > .5\n",
    "    return np.mean(results == y)\n",
    "\n",
    "def test_reg_cat(epochs, lambd = 0) :\n",
    "    np.random.seed(0)\n",
    "    activations = ['relu', 'relu', 'sigmoid']\n",
    "    params, costs = train_model(x_train_cat_norm, y_train_cat, [64*64*3, 20, 7, 1], activations,\n",
    "                            ['he', 'he', 'he'], lambd = lambd, batch_size = 256,\n",
    "                            epochs = epochs, learning_rate = 0.01, show_cost = False)\n",
    "    print('Accuracy on training set : %f%%' % (100*accuracy_cat(x_train_cat_norm, y_train_cat, params, activations)))\n",
    "    print('Accuracy on test set : %f%%'     % (100*accuracy_cat(x_test_cat_norm , y_test_cat , params, activations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 99.521531%\n",
      "Accuracy on test set : 70.000000%\n"
     ]
    }
   ],
   "source": [
    "test_reg_cat(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 99.043062%\n",
      "Accuracy on test set : 72.000000%\n"
     ]
    }
   ],
   "source": [
    "test_reg_cat(200, lambd = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 99.043062%\n",
      "Accuracy on test set : 76.000000%\n"
     ]
    }
   ],
   "source": [
    "test_reg_cat(200, lambd = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 99.043062%\n",
      "Accuracy on test set : 78.000000%\n"
     ]
    }
   ],
   "source": [
    "test_reg_cat(200, lambd = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 97.607656%\n",
      "Accuracy on test set : 76.000000%\n"
     ]
    }
   ],
   "source": [
    "test_reg_cat(200, lambd = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 65.550239%\n",
      "Accuracy on test set : 34.000000%\n"
     ]
    }
   ],
   "source": [
    "test_reg_cat(200, lambd = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là, pour le coup, c'est mieux. On n'arrive pas à réduire tout l'overfit à coup de régularisation L2, mais enfin on peut gagner jusqu'à 8 points en test pour le même réseau !\n",
    "\n",
    "Par contre, à la fin, on voit aussi que trop, c'est trop :)\n",
    "\n",
    "D'autres méthodes pour contrer l'overfit existent, comme le dropout, ou tout simplement avoir plus de données (y'a que 200 chats en training...), un réseau moins large ou moins profond, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
