<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr" lang="fr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/> 
		<title>Codeur confiné saison 2</title>
	</head>
	<body>
		<a href="J01-Régression linéaire et équation normale.html">J01-Régression linéaire et équation normale</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2, c'est parti !<br/>
			<br/>
			Comme annoncé, la saison 2 du Codeur confiné sera orientée Machine Learning.<br/>
			Et comme malheureusement JavaScript n'est pas le meilleur outil pour ça, le format "algorithme + démo interactive" change aussi...<br/>
			<br/>
			A la place, ça sera du Python et des notebooks Jupyter. Il faut ce qu'il faut...<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce premier jour, définition de la régression linéaire et de l'équation normale.<br/>
		</div><br/>
		<a href="J02-Régression polynomiale.html">J02-Régression polynomiale</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 2, ça continue - toujours orientée Machine Learning.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce second jour, application de la régression linéaire à des cas non-linéaires.<br/>
		</div><br/>
		<a href="J03-Descente de gradient.html">J03-Descente de gradient</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 3, Machine Learning toujours.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce troisième jour, un des algorithmes des plus importants de tout le domaine ML : la descente de gradient (qui n'est d'ailleurs pas dédiée au modèle linéaire)<br/>
		</div><br/>
		<a href="J04-Normalisation.html">J04-Normalisation</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 4, Machine Learning toujours.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce quatrième jour, une technique : la normalisation des données, qui est applicable à beaucoup de modèles et pas uniquement la régression linéaire - c'est une des méthodes piliers pour optimiser la vitesse d'apprentissage !<br/>
		</div><br/>
		<a href="J05-Descente batch, mini batch et stochastique.html">J05-Descente batch, mini batch et stochastique</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 5, Machine Learning.<br/>
			<br/>
			Pour cette fin de première semaine, un dernier sujet lié à la régression linéaire, mais pas que : la descente mini-batch. Comme la descente de gradient, c'est une technique clef dans la plupart des algorithmes ML, qui permet de traiter des larges volumes.<br/>
		</div><br/>
		<a href="J06-Regression logistique.html">J06-Regression logistique</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 6<br/>
			<br/>
			On attaque cette semaine les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Alerte spoiler : Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaitre des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			Pour bien attaquer, sans griller les étapes, présentation aujourd’hui de la régression logistique : ça commence comme une régression linéaire, mais c’est mieux sur la fin :D<br/>
		</div><br/>
		<a href="J07-Test, biais et variance, régularisation - partie 1.html">J07-Test, biais et variance, régularisation - partie 1</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 7<br/>
			<br/>
			On continue sur le sujet de cette semaine, les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Alerte spoiler (quoique je l’ai déjà dit hier): Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaitre des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			Sauf que pour parler de performance d’un modèle, encore faut-il définir ce que c’est, et ce qui pourrait le plomber dans un premier temps, et par la suite comment rectifier le tir au besoin.<br/>
			Le sujet est vaste, il sera sur deux jours : « Test, biais et variance, régularisation » avec introduction de l’overfit dont on parle souvent dans l’apprentissage machine.<br/>
		</div><br/>
		<a href="J08-Test, biais et variance, régularisation - partie 2.html">J08-Test, biais et variance, régularisation - partie 2</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 8<br/>
			<br/>
			On continue sur le sujet de cette semaine, les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Rappel : Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaître des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			La suite de la suite « Test, biais et variance, régularisation » avec la manière de base pour contrer l’overfit: la régularisation.<br/>
		</div><br/>
		<a href="J09-Classification multi-classes.html">J09-Classification multi-classes</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 9<br/>
			<br/>
			Toujours sur le sujet de la semaine, les problèmes de classifications supervisées, on va attaquer les véritables cas pratiques. Comme annoncé depuis le début de la semaine, aujourd’hui on vas lire les caractères manuscrits!<br/>
			<br/>
			Avec quelques lignes de code et quelques minutes d’entrainement, on va utiliser la régression logistique pour reconnaitre des chiffres.<br/>
		</div><br/>
		<a href="J10-Classification kNN.html">J10-Classification kNN</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 10<br/>
			<br/>
			Un dernier sujet sur la classification supervisée (avant qu'on attaque les réseaux de neurones plus tard en tous les cas). <br/>
			On va aujourd'hui résoudre le même problème, mais différement : via l'utilisation des "K plus proches voisins"<br/>
			<br/>
			L'algorithme on le verra fait quelques lignes, pas plus, rien de compliqué ça pourrait être un TP de première semaine de programmation quasiment, et pourtant on va taper le 95% de reconnaissance! <br/>
		</div><br/>
	</body>
</html>
