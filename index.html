<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr" lang="fr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/> 
		<title>Codeur confiné saison 2</title>
	</head>
	<body>
		<a href="J01-Régression linéaire et équation normale.html">J01-Régression linéaire et équation normale</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2, c'est parti !<br/>
			<br/>
			Comme annoncé, la saison 2 du Codeur confiné sera orientée Machine Learning.<br/>
			Et comme malheureusement JavaScript n'est pas le meilleur outil pour ça, le format "algorithme + démo interactive" change aussi...<br/>
			<br/>
			A la place, ça sera du Python et des notebooks Jupyter. Il faut ce qu'il faut...<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce premier jour, définition de la régression linéaire et de l'équation normale.<br/>
		</div><br/>
		<a href="J02-Régression polynomiale.html">J02-Régression polynomiale</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 2, ça continue - toujours orientée Machine Learning.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce second jour, application de la régression linéaire à des cas non-linéaires.<br/>
		</div><br/>
		<a href="J03-Descente de gradient.html">J03-Descente de gradient</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 3, Machine Learning toujours.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce troisième jour, un des algorithmes des plus importants de tout le domaine ML : la descente de gradient (qui n'est d'ailleurs pas dédiée au modèle linéaire)<br/>
		</div><br/>
		<a href="J04-Normalisation.html">J04-Normalisation</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 4, Machine Learning toujours.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce quatrième jour, une technique : la normalisation des données, qui est applicable à beaucoup de modèles et pas uniquement la régression linéaire - c'est une des méthodes piliers pour optimiser la vitesse d'apprentissage !<br/>
		</div><br/>
		<a href="J05-Descente batch, mini batch et stochastique.html">J05-Descente batch, mini batch et stochastique</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 5, Machine Learning.<br/>
			<br/>
			Pour cette fin de première semaine, un dernier sujet lié à la régression linéaire, mais pas que : la descente mini-batch. Comme la descente de gradient, c'est une technique clef dans la plupart des algorithmes ML, qui permet de traiter des larges volumes.<br/>
		</div><br/>
		<a href="J06-Regression logistique.html">J06-Regression logistique</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 6<br/>
			<br/>
			On attaque cette semaine les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Alerte spoiler : Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaitre des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			Pour bien attaquer, sans griller les étapes, présentation aujourd’hui de la régression logistique : ça commence comme une régression linéaire, mais c’est mieux sur la fin :D<br/>
		</div><br/>
		<a href="J07-Test, biais et variance, régularisation - partie 1.html">J07-Test, biais et variance, régularisation - partie 1</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 7<br/>
			<br/>
			On continue sur le sujet de cette semaine, les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Alerte spoiler (quoique je l’ai déjà dit hier): Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaitre des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			Sauf que pour parler de performance d’un modèle, encore faut-il définir ce que c’est, et ce qui pourrait le plomber dans un premier temps, et par la suite comment rectifier le tir au besoin.<br/>
			Le sujet est vaste, il sera sur deux jours : « Test, biais et variance, régularisation » avec introduction de l’overfit dont on parle souvent dans l’apprentissage machine.<br/>
		</div><br/>
		<a href="J08-Test, biais et variance, régularisation - partie 2.html">J08-Test, biais et variance, régularisation - partie 2</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 8<br/>
			<br/>
			On continue sur le sujet de cette semaine, les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Rappel : Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaître des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			La suite de la suite « Test, biais et variance, régularisation » avec la manière de base pour contrer l’overfit: la régularisation.<br/>
		</div><br/>
		<a href="J09-Classification multi-classes.html">J09-Classification multi-classes</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 9<br/>
			<br/>
			Toujours sur le sujet de la semaine, les problèmes de classifications supervisées, on va attaquer les véritables cas pratiques. Comme annoncé depuis le début de la semaine, aujourd’hui on vas lire les caractères manuscrits!<br/>
			<br/>
			Avec quelques lignes de code et quelques minutes d’entrainement, on va utiliser la régression logistique pour reconnaitre des chiffres.<br/>
		</div><br/>
		<a href="J10-Classification kNN.html">J10-Classification kNN</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 10<br/>
			<br/>
			Un dernier sujet sur la classification supervisée (avant qu'on attaque les réseaux de neurones plus tard en tous les cas). <br/>
			On va aujourd'hui résoudre le même problème, mais différement : via l'utilisation des "K plus proches voisins"<br/>
			<br/>
			L'algorithme on le verra fait quelques lignes, pas plus, rien de compliqué ça pourrait être un TP de première semaine de programmation quasiment, et pourtant on va taper le 95% de reconnaissance! <br/>
		</div><br/>
		<a href="J11-K means.html">J11-K means</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 11<br/>
			<br/>
			Cette semaine, on attaque les sujets de machine learning "non-supervisé". En un mot, je n'ai pas moyen de montrer à la machine ce que j'attends à partir d'exemples (la supervision), mais je souhaite quand même exploiter au mieux les données.<br/>
			Comme on va le voir, les algorithmes sont souvent très différents du monde supervisé, et pour atteindre des objectifs différents.<br/>
			Aujourd'hui, on démarre avec un must : l'algorithme des K-means, ou comment classer au mieux un tas de données inconnues<br/>
			Il s'agit encore une fois d'un algorithme puissant qui tient en quelques lignes ! <br/>
			<br/>
			Et pour le fun, on va l'appliquer à la compression d'image :)<br/>
		</div><br/>
		<a href="J12-Détection d'anomalies.html">J12-Détection d'anomalies</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 12<br/>
			<br/>
			On continue sur les sujets de la semaine : les algorithmes de machine learning non-supervisée.<br/>
			Avec un algorithme assez classique aujourd'hui : la détection d'erreurs. Parce que la plus grosse fraude est celle qu'on ne voit pas venir, parce qu'une erreur imprévue est rarement connue à l'avance : comment reconnaitre une erreur par rapport à des données "habituelles".<br/>
			Distributions gaussiennes au programme : et vivent les cloches :D <br/>
			Et encore une fois, l'algorithme est vraiment court et simple !<br/>
		</div><br/>
		<a href="J13-Analyse en composantes principales.html">J13-Analyse en composantes principales</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 13<br/>
			<br/>
			Un autre algorithme de machine learning non supervisé aujourd'hui : l'analyse en composantes principales.<br/>
			En un mot : comment trouver une représentation de mes données, même approximative, qui les comprime au mieux. Les applications sont, par exemple, de pouvoir entrainer des modèles plus simples, avec des données un peu nettoyées.<br/>
			On verra notamment que les images de chiffres manuscrits utilisées les jours derniers (28x28 = 784 octets) pourraient tout à fait tenir sur beaucoup moins !<br/>
			Il y aura pas mal d'introduction avec des concepts d'algèbre linéaire - mais rien de vraiment hyper compliqué et on peut toujours utiliser la méthode même sans avoir compris cette partie au pire.<br/>
			Et même si l'algorithme n'est pas simple à comprendre, on peut néanmoins le coder en une ligne, avec numpy<br/>
		</div><br/>
		<a href="J14-Classification ascendante hierarchique.html">J14-Classification ascendante hierarchique</a><br/>
                <div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 14<br/>
                        <br/>
			Toujours du machine learning non-supervisé, c'est le sujet de la semaine. Aujourd'hui, une classification un peu différente des k-means : comment trouver la meilleure hiérarchie.<br/>
			Si Darwin avait eu cet algorithme, il aurait pu l'alimenter avec toutes ses données sur les différentes espèces et sortir une classification pour sa théorie de l'évolution qui aurait été "optimale", suivant certains critères.<br/>
			En un mot, on va apprendre à classer les données dans un arbre où la proximité indique une ressemblance.<br/>
			La mise en situation aujourd'hui est assez cocasse, c'est une classification hiérarchique... de fromages :)<br/>
			Et exceptionnellement, on triche et on utilise scipy aujourd'hui. Avec cette libraire, l'algorithme fait... une ligne :)<br/>
                </div><br/>
		<a href="J15-Régressions et fonctions non linéaires.html">J15-Régressions et fonctions non linéaires</a><br/>
                <div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 15<br/>
                        <br/>
			Pour terminer cette semaine de machine learning non-supervisé, un sujet un peu atypique.<br/>
			Il s'agit d'une régression logistique (donc supervisée) qui n'a aucune chance d'aboutir directement : il faudra nécessairement prétraiter les entrées<br/>
			Et on va le voir, le système pourra apprendre à trouver tout seul le meilleur pré-traitement des entrées en amont.<br/>
			<br/>
			C'est aussi un mini-réseau de neurones, dont on parlera plus en détail la semaine prochaine.<br/>
			On se retrouve Lundi pour la suite - semaine consacrée aux réseaux de neurones et à leurs optimisations<br/>
                </div><br/>
		<a href="J16-Réseaux de neurones.html">J16-Réseaux de neurones</a><br/>
                <div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 16<br/>
                        <br/>
			C'est parti pour une quatrième semaine du codeur confiné. Cette semaine : réseaux de neurones (enfin).<br/>
			Et vers la fin on mettra même le pied dans le deep learning.<br/>
			<br/>
			Pour l'instant aujourd'hui, on va découvrir ce qu'est un réseau de neurones, la rétro-propagation du gradient, etc...<br/>
			Et en activité pratique, on va revenir sur la reconnaissance de chiffres manuscrits avec un algorithme rapide qui va dépasser les 90% cette fois-ci.<br/>
			<br/>
			A suivre cette semaine : les fonctions d'activations, le gradient check, le deep learning, et la reconnaissance d'images.<br/>
                </div><br/>
		<a href="J17-Fonctions d'activation.html">J17-Fonctions d'activation</a><br/>
                <div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 17<br/>
                        <br/>
			On continue sur le sujet de cette quatrième semaine : les réseaux de neurones.<br/>
			La base a été posée hier, maintenant on va essayer de trouver des méthodes pour converger plus vite : réduire le temps de calcul d'une itération par exemple, ou converger en moins d'itérations.<br/>
			<br/>
			Un des concepts clef est de bien choisir sa fonction d'activation : il n'y a pas que la sigmoïde dans la vie, et on va découvrir d'autres options.<br/>
			<br/>
			Coming next : le gradient check, un peu de deep learning, et la reconnaissance d'images.<br/>
                </div><br/>
		<a href="J18-Validation du gradient.html">J18-Validation du gradient</a><br/>
                <div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 18<br/>
                        <br/>
			La suite du sujet de la semaine 4, réseaux de neurones : le gradient check - ou validation du gradient.<br/>
			Le sujet est plutôt court, et ça n'est pas une méthode de Machine Learning, juste un outil de debug.<br/>
			En un mot, les calculs de dérivées à travers toutes les étapes d'un réseau de neurones sont complexes et sujets à des erreurs d'implémentations. Ajouter des nouvelles fonctions d'activation, comme hier, n'aide pas à simplifier. Et passer sur des réseaux plus profonds, comme on le fera demain, va nous achever si on n'est pas nickel sur ces calculs.<br/>
			<br/>
			D'où la méthode de validation du gradient. Comment obtenir une bonne approximation de ce que devrait être un gradient.<br/>
			C'est une méthode qui sert à toutes les descentes de gradient en fait, et pas uniquement aux réseaux de neurones. Simplement, là, on enchaine les fonctions et les dérivées, dans tous les sens, et ça va empirer : un outil qui valide nos calculs de dérivées sera bien utile !<br/>
			<br/>
			Coming next cette semaine : un peu de deep learning, et la reconnaissance d'images.<br/>
                </div><br/>
		<a href="J19-Réseau profond.html">J19-Réseau profond</a><br/>
                <div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 19<br/>
                        <br/>
			Grace aux réseaux de neurones, sujet de la semaine, on va pouvoir mettre le pied dans le Deep Learning, ou apprentissage profond.<br/>
			Comment enchaîner les couches d'entraînement pour arriver à des modèles bien meilleurs qu'une régression simple ou un modèle avec une seule couche intermédiaire.<br/>
			On va friser le 97% de reconnaissance sur notre jeu de chiffres manuscrits !<br/>
			<br/>
			Pour finir cette semaine, on verra demain comment l'appliquer à d'autres sujets, comme la reconnaissance d'image par exemple.<br/>
                </div><br/>
		<a href="J20-Reconnaissance d'image.html">J20-Reconnaissance d'image</a><br/>
                <div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 20<br/>
                        <br/>
			Un sujet plutôt court aujourd'hui pour finir cette semaine sur les réseaux de neurones. Ou en tous les cas, une simple application des 20 derniers jours.<br/>
			Petit à petit, on a réussi à mettre en place des briques, on va maintenant utiliser le tout sur un problème de reconnaissance d'images simple : uniquement du réseau de neurones, pour reconnaître une image cible (des chats, je sais pas pourquoi mais c'est tendance dans le domaine du Machine Learning).<br/>
			Bref donc, un détecteur de chats à base de réseaux de neurones profonds, sans entrer dans les méandres de réseaux de convolution ou autres choses plus compliquées.
			<br/>
			Note : la semaine prochaine, le confinement s'arrête, et la saison 2 aussi :)<br/>
			Cependant j'ai encore deux sujets sur le feu que j'aurais voulu faire rentrer : le Codeur Confiné finira donc sa saison mardi prochain !<br/>
			On verra donc dès lundi deux concepts clefs pour améliorer le training : bien initialiser ses paramètres, et bien détecter / traiter de l'overfit.<br/>
                </div><br/>
	</body>
</html>
