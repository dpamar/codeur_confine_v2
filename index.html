<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr" lang="fr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/> 
		<title>Codeur confiné saison 2</title>
	</head>
	<body>
		<a href="J01-Régression linéaire et équation normale.html">J01-Régression linéaire et équation normale</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2, c'est parti !<br/>
			<br/>
			Comme annoncé, la saison 2 du Codeur confiné sera orientée Machine Learning.<br/>
			Et comme malheureusement JavaScript n'est pas le meilleur outil pour ça, le format "algorithme + démo interactive" change aussi...<br/>
			<br/>
			A la place, ça sera du Python et des notebooks Jupyter. Il faut ce qu'il faut...<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce premier jour, définition de la régression linéaire et de l'équation normale.<br/>
		</div><br/>
		<a href="J02-Régression polynomiale.html">J02-Régression polynomiale</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 2, ça continue - toujours orientée Machine Learning.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce second jour, application de la régression linéaire à des cas non-linéaires.<br/>
		</div><br/>
		<a href="J03-Descente de gradient.html">J03-Descente de gradient</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 3, Machine Learning toujours.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce troisième jour, un des algorithmes des plus importants de tout le domaine ML : la descente de gradient (qui n'est d'ailleurs pas dédiée au modèle linéaire)<br/>
		</div><br/>
		<a href="J04-Normalisation.html">J04-Normalisation</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 4, Machine Learning toujours.<br/>
			<br/>
			Pour cette première semaine, d'introduction et de bases du ML, je propose de regarder en détail un modèle simple : la régression linéaire.<br/>
			<br/>
			Et pour ce quatrième jour, une technique : la normalisation des données, qui est applicable à beaucoup de modèles et pas uniquement la régression linéaire - c'est une des méthodes piliers pour optimiser la vitesse d'apprentissage !<br/>
		</div><br/>
		<a href="J05-Descente batch, mini batch et stochastique.html">J05-Descente batch, mini batch et stochastique</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 5, Machine Learning.<br/>
			<br/>
			Pour cette fin de première semaine, un dernier sujet lié à la régression linéaire, mais pas que : la descente mini-batch. Comme la descente de gradient, c'est une technique clef dans la plupart des algorithmes ML, qui permet de traiter des larges volumes.<br/>
		</div><br/>
		<a href="J06-Regression logistique.html">J06-Regression logistique</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 6<br/>
			<br/>
			On attaque cette semaine les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Alerte spoiler : Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaitre des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			Pour bien attaquer, sans griller les étapes, présentation aujourd’hui de la régression logistique : ça commence comme une régression linéaire, mais c’est mieux sur la fin :D<br/>
		</div><br/>
		<a href="J07-Test, biais et variance, régularisation - partie 1.html">J07-Test, biais et variance, régularisation - partie 1</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 7<br/>
			<br/>
			On continue sur le sujet de cette semaine, les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Alerte spoiler (quoique je l’ai déjà dit hier): Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaitre des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			Sauf que pour parler de performance d’un modèle, encore faut-il définir ce que c’est, et ce qui pourrait le plomber dans un premier temps, et par la suite comment rectifier le tir au besoin.<br/>
			Le sujet est vaste, il sera sur deux jours : « Test, biais et variance, régularisation » avec introduction de l’overfit dont on parle souvent dans l’apprentissage machine.<br/>
		</div><br/>
		<a href="J08-Test, biais et variance, régularisation - partie 2.html">J08-Test, biais et variance, régularisation - partie 2</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 8<br/>
			<br/>
			On continue sur le sujet de cette semaine, les problèmes de classifications supervisées. Comment trouver à quelle classe appartient un objet que je n’ai jamais vu auparavant.<br/>
			<br/>
			Rappel : Avant la fin de la semaine, on aura vu deux algorithmes pour reconnaître des caractères manuscrits - dont un qui tient en une poignée de ligne avec un taux de succès &gt; 95 % !<br/>
			<br/>
			La suite de la suite « Test, biais et variance, régularisation » avec la manière de base pour contrer l’overfit: la régularisation.<br/>
		</div><br/>
		<a href="J09-Classification multi-classes.html">J09-Classification multi-classes</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 9<br/>
			<br/>
			Toujours sur le sujet de la semaine, les problèmes de classifications supervisées, on va attaquer les véritables cas pratiques. Comme annoncé depuis le début de la semaine, aujourd’hui on vas lire les caractères manuscrits!<br/>
			<br/>
			Avec quelques lignes de code et quelques minutes d’entrainement, on va utiliser la régression logistique pour reconnaitre des chiffres.<br/>
		</div><br/>
		<a href="J10-Classification kNN.html">J10-Classification kNN</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 10<br/>
			<br/>
			Un dernier sujet sur la classification supervisée (avant qu'on attaque les réseaux de neurones plus tard en tous les cas). <br/>
			On va aujourd'hui résoudre le même problème, mais différement : via l'utilisation des "K plus proches voisins"<br/>
			<br/>
			L'algorithme on le verra fait quelques lignes, pas plus, rien de compliqué ça pourrait être un TP de première semaine de programmation quasiment, et pourtant on va taper le 95% de reconnaissance! <br/>
		</div><br/>
		<a href="J11-K means.html">J11-K means</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 11<br/>
			<br/>
			Cette semaine, on attaque les sujets de machine learning "non-supervisé". En un mot, je n'ai pas moyen de montrer à la machine ce que j'attends à partir d'exemples (la supervision), mais je souhaite quand même exploiter au mieux les données.<br/>
			Comme on va le voir, les algorithmes sont souvent très différents du monde supervisé, et pour atteindre des objectifs différents.<br/>
			Aujourd'hui, on démarre avec un must : l'algorithme des K-means, ou comment classer au mieux un tas de données inconnues<br/>
			Il s'agit encore une fois d'un algorithme puissant qui tient en quelques lignes ! <br/>
			<br/>
			Et pour le fun, on va l'appliquer à la compression d'image :)<br/>
		</div><br/>
		<a href="J12-Détection d'anomalies.html">J12-Détection d'anomalies</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 12<br/>
			<br/>
			On continue sur les sujets de la semaine : les algorithmes de machine learning non-supervisée.<br/>
			Avec un algorithme assez classique aujourd'hui : la détection d'erreurs. Parce que la plus grosse fraude est celle qu'on ne voit pas venir, parce qu'une erreur imprévue est rarement connue à l'avance : comment reconnaitre une erreur par rapport à des données "habituelles".<br/>
			Distributions gaussiennes au programme : et vivent les cloches :D <br/>
			Et encore une fois, l'algorithme est vraiment court et simple !<br/>
		</div><br/>
		<a href="J13-Analyse en composantes principales.html">J13-Analyse en composantes principales</a><br/>
		<div style="padding: 10; border: solid black 1px">Codeur confiné, saison 2 - jour 13<br/>
			<br/>
			Un autre algorithme de machine learning non supervisé aujourd'hui : l'analyse en composantes principales.<br/>
			En un mot : comment trouver une représentation de mes données, même approximative, qui les comprime au mieux. Les applications sont, par exemple, de pouvoir entrainer des modèles plus simples, avec des données un peu nettoyées.<br/>
			On verra notamment que les images de chiffres manuscrits utilisées les jours derniers (28x28 = 784 octets) pourraient tout à fait tenir sur beaucoup moins !<br/>
			Il y aura pas mal d'introduction avec des concepts d'algèbre linéaire - mais rien de vraiment hyper compliqué et on peut toujours utiliser la méthode même sans avoir compris cette partie au pire.<br/>
			Et même si l'algorithme n'est pas simple à comprendre, on peut néanmoins le coder en une ligne, avec numpy<br/>
		</div><br/>
	</body>
</html>
