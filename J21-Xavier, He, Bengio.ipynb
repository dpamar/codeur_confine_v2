{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](brain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier, He, Bengio\n",
    "## Mise en situation\n",
    "\n",
    "J'ai un réseau mutlticouches : ReLU, ReLU, ReLU, ..., et Sigmoïde à la fin.\n",
    "\n",
    "Je suis incapable de faire tourner une descente de gradient. Ça plante avant même la première étape, et pourtant mon code est bon !\n",
    "\n",
    "## L'explosion / la disparition du signal\n",
    "On a mentionné très brièvement le principe déjà, mais voilà le détail.\n",
    "\n",
    "### W trop grands\n",
    "On va considérer pour l'exemple un réseau de taille \"Input 64x64x3\" / \"ReLU 20\" / \"ReLU 10\" / \"ReLU 5\" / \"Sigmoid 1\" .\n",
    "\n",
    "Supposons que je prenne mes valeurs initiales de W entre 0 et 1, uniformément : la valeur moyenne est de 0.5 statistiquement. Prenons aussi une donnée d'exemple avec toutes les mesures égales à 1.\n",
    "\n",
    "* Sur le passage de la première couche\n",
    "  * Z va sommer 64x64x3 = 12288 termes. Total moyen : 6144 en moyenne sur chacun des 20 neurones.\n",
    "  * ReLU : on à 6144 en sortie\n",
    "* Couche suivante :\n",
    "  * Calcul de Z : 6144 x 20 termes qui valent 0.5 en moyenne, total : 61440\n",
    "  * ReLU : 61440\n",
    "* Couche suivante (j'abrège) : 307200 en moyenne\n",
    "* Sigmoide finale : s(768000) ~= 1\n",
    "\n",
    "Le résultat est $1 - \\epsilon$ avec $\\epsilon ~= 0.000000000...0000....000....0006$, enfin bref il va falloir plus de 330.000 zéros avant d'avoir un chiffre significatif dans $\\epsilon$. **Autant dire que la sigmoide vaut 1, point barre**, en tout cas Python le définira comme tel.\n",
    "\n",
    "Arrive le calcul du gradient, qu'on commence sur la dernière couche : $dA = -\\frac{1}{m} (\\frac{Y}{A} - \\frac{1-Y}{1-A})$ : la deuxième fraction avec son dénominateur 1-A va faire une division par 0 (ou presque si on est puriste, mais pour Python ça ne fera pas une grosse différence...)\n",
    "\n",
    "### W trop petits\n",
    "Mauvaise solution : je divise tous mes poids de départ par 500 millions et je suis tranquille. Valeur moyenne d'un poids : 1 milliardième.\n",
    "\n",
    "Le résultat, c'est que le signal risque de disparaitre : Prenons \"Input 2\" / \"ReLU 2\" / \"ReLU 2\" / \"ReLU 2\" / \"ReLU 2\" / \"Sigmoid 1\" :\n",
    "* en sortie de la première couche : Z = A = 2e-9 en moyenne\n",
    "* en sortie de la seconde : Z = A = 4e-18\n",
    "* puis Z = A = 8e-27\n",
    "* puis Z = A = 16e-36\n",
    "* en sortie de la dernière : 0.5 à un micro pouillème près - parce que 0.000000........016 ou rien c'est pareil...\n",
    "\n",
    "Il faut donc trouver une méthode smart d'initialisation\n",
    "\n",
    "## Initialion des poids\n",
    "### Distribution\n",
    "Déjà, au lieu de prendre une distribution uniforme, on va prendre une distribution normale. Ça permet d'être plus exact quand on fait des hypothèses du genre \"la moyenne devrait être autour de \" etc...\n",
    "\n",
    "Pour rappel, une distribution est définie par sa moyenne $\\mu$ et sa variance $\\sigma$.\n",
    "\n",
    "### Initialisation de Xavier\n",
    "Très utile dans les tanh : il est nécessaire d'avoir des valeurs pas trop éloignées de 0 pour que la tanh renvoie autre chose que 1 ou -1 (au pouillème près).\n",
    "\n",
    "Les $n$ données d'entrée étant elles-mêmes normalisées, il a été montré que prendre une variance $\\frac{1}{n}$ permettait d'alimenter tanh avec des valeurs correctes. Il faut donc, pour obtenir cette variance, diviser les valeurs aléatoire obtenues par l'écart-type, soit $\\frac{1}{\\sqrt{n}}$\n",
    "\n",
    "### Initialisation de He\n",
    "Une variante, qui marche mieux pour les fonctions ReLU : appliquer un facteur 2 à la variance de Xavier, et donc un facteur $\\sqrt{2}$ à l'écart-type : $\\sqrt{\\frac{2}{n}}$\n",
    "\n",
    "### Initialisation de Bengio\n",
    "De Xavier Glorot et Bengio, en fait; mais comme la première méthode elle-même est déjà dénommée d'après Xavier Glorot... on s'y perd :)\n",
    "\n",
    "Bref, là, l'idée est de prendre cette fois $\\frac{1}{\\sqrt{n_1+n_2}}$, avec $n_1$ la taille de la couche précédente (taille des entrées) et $n_2$ la couche elle-même (taille de la sortie)\n",
    "\n",
    "### Autres\n",
    "Vu que ces découvertes sont assez récentes, il risque fort d'y en avoir d'autres dans un avenir pas si lointain :)\n",
    "\n",
    "## Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'initialisation\n",
    "def uniform(in_dim, out_dim): return np.random.rand(out_dim, in_dim)\n",
    "def uniform_100(in_dim, out_dim): return uniform(in_dim, out_dim) * 0.01\n",
    "def gaussian(in_dim, out_dim):return np.random.randn(out_dim, in_dim)\n",
    "def xavier(in_dim, out_dim): return gaussian(in_dim, out_dim) / np.sqrt(in_dim)\n",
    "def he(in_dim, out_dim): return gaussian(in_dim, out_dim) * np.sqrt(2/in_dim)\n",
    "def bengio(in_dim, out_dim): return gaussian(in_dim, out_dim) / np.sqrt(out_dim + in_dim)\n",
    "\n",
    "init_functions = {'uniform'   : uniform,\n",
    "                 'uniform_100': uniform_100,\n",
    "                 'gaussian'   : gaussian,\n",
    "                 'xavier'     : xavier,\n",
    "                 'he'         : he,\n",
    "                 'bengio'     : bengio}\n",
    "\n",
    "# Les différentes fonctions\n",
    "def sigmoid(x) : return 1 / (1 + np.exp(-x))\n",
    "def tanh(x): return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "def relu(x): return np.maximum(x, 0)\n",
    "\n",
    "act_functions = {'sigmoid': sigmoid, 'tanh' : tanh, 'relu' : relu}\n",
    "\n",
    "# Leurs dérivées\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def d_tanh(x):\n",
    "    t = tanh(x)\n",
    "    return 1 - t**2\n",
    "\n",
    "def d_relu(x):\n",
    "    return x > 0\n",
    "\n",
    "act_derivates = {'sigmoid': d_sigmoid, 'tanh' : d_tanh, 'relu' : d_relu}\n",
    "\n",
    "# Passe en avant : 1 couche - on utilise le dictionnaire de fonctions\n",
    "def layer_forward_pass(X, W, b, activation):\n",
    "    Z = np.dot(W, X) + b\n",
    "    A = act_functions[activation](Z)\n",
    "    return Z, A\n",
    "\n",
    "# Passe en avant : toutes les couches\n",
    "def model_forward_pass(X, activations, parameters):\n",
    "    result = {}\n",
    "    result['A0'] = X\n",
    "    # Entrée de la première couche: X\n",
    "    A = X\n",
    "    for i in range(1, len(activations) + 1):\n",
    "        # Pour chaque couche, une passe en avant. Les W et b viennent de parameters\n",
    "        Z_next, A_next = layer_forward_pass(A, parameters['W' + str(i)], parameters['b' + str(i)], activations[i-1])\n",
    "        result['Z' + str(i)] = Z_next\n",
    "        result['A' + str(i)] = A_next\n",
    "        A = A_next\n",
    "    return result\n",
    "\n",
    "# Passe en arrière : 1 couche - on utilise le dictionnaire de dérivées\n",
    "def layer_backward_pass(dA, Z, A_prev, W, activation):\n",
    "    dZ = dA * act_derivates[activation](Z)\n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dW, db, dA_prev\n",
    "\n",
    "# Passe en arrière : toutes les couches\n",
    "def model_backward_pass(dA_last, parameters, forward_pass_results, activations):\n",
    "    gradients = {}\n",
    "    dA = dA_last\n",
    "    for i in range(len(activations), 0, -1):\n",
    "        dW, db, dA_prev = layer_backward_pass(dA,\n",
    "                                              forward_pass_results['Z' + str(i)],\n",
    "                                              forward_pass_results['A' + str(i-1)],\n",
    "                                              parameters['W' + str(i)],\n",
    "                                              activations[i-1])\n",
    "        gradients['dW' + str(i)] = dW\n",
    "        gradients['db' + str(i)] = db\n",
    "        dA = dA_prev\n",
    "    return gradients\n",
    "\n",
    "def train_model(X, Y, layer_dimensions, layer_activations, initializations,\n",
    "               learning_rate = 0.01, epochs = 10, batch_size = 64,\n",
    "               show_cost = False):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    #Nombre de couches - hors celle des entrées\n",
    "    l = len(layer_dimensions)-1\n",
    "    \n",
    "    # Création de tous les paramètres\n",
    "    # A chaque étape, W a pour dimensions \"nb neurones de la couche\" x \"nb entrées\"\n",
    "    # Et b est un vecteur, une valeur par neurone\n",
    "    parameters = {}\n",
    "    for i in range(1, l+1):\n",
    "        parameters['W' + str(i)] = init_functions[initializations[i-1]](layer_dimensions[i-1], layer_dimensions[i])\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dimensions[i], 1))\n",
    "    \n",
    "    costs = []\n",
    "    # Apprentissage\n",
    "    for e in range(epochs):\n",
    "        for s in range(0, m, batch_size):\n",
    "            x_batch = X[:, s:s+batch_size]\n",
    "            y_batch = Y[:, s:s+batch_size]\n",
    "    \n",
    "            # Passe en avant\n",
    "            forward_pass_results = model_forward_pass(x_batch, layer_activations, parameters)\n",
    "            \n",
    "            # Calcul de la dérivée du coût par rapport au dernier A\n",
    "            A_last = forward_pass_results['A' + str(l)]\n",
    "            dA_last = -(np.divide(y_batch, A_last) - np.divide(1 - y_batch, 1 - A_last))/x_batch.shape[1]\n",
    "\n",
    "            # Calcul des gradients - passe en arrière\n",
    "            gradients = model_backward_pass(dA_last, parameters, forward_pass_results, layer_activations)\n",
    "            \n",
    "            # Descente de gradient\n",
    "            for i in range(1, l+1):\n",
    "                parameters['W' + str(i)] -= learning_rate * gradients['dW' + str(i)]\n",
    "                parameters['b' + str(i)] -= learning_rate * gradients['db' + str(i)]\n",
    "        \n",
    "        # Un peu de debug\n",
    "        model_result = model_forward_pass(X, layer_activations, parameters)['A' + str(l)]\n",
    "        cost = np.squeeze(-np.sum(np.log(model_result) * Y + np.log(1 - model_result) * (1-Y))/m)\n",
    "        costs.append(cost)\n",
    "        if show_cost : print('Epoch #%i: %s' % (e+1, cost))\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retour à la mise en situation\n",
    "\n",
    "### Chargement des données\n",
    "\n",
    "On continue avec le dataset de Yann Le Cun http://yann.lecun.com/exdb/mnist/ (images 28x28, 60.000 données d'entrainement et 10.000 données de validation), et on va regarder les impacts sur un réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file): \n",
    "    data = np.load(file)\n",
    "    return data['x'], data['y']\n",
    "\n",
    "x_train, y_train = load('data/d09_train_data.npz')\n",
    "x_test , y_test  = load('data/d09_test_data.npz')\n",
    "\n",
    "mus    = x_train.mean(axis = 0, keepdims = True)\n",
    "sigmas = x_train.std (axis = 0, keepdims = True) + 1e-9\n",
    "\n",
    "x_train_norm = (x_train-mus)/sigmas\n",
    "x_test_norm  = (x_test -mus)/sigmas\n",
    "\n",
    "y_train_mat = (y_train == np.arange(10)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allez, c'est parti. On va reprendre notre réseau pas trop mal (cf jour 19) : \"relu 50 / relu 25 / sigmoide 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['relu', 'relu', 'sigmoid']\n",
    "\n",
    "def test_initializations(initializations, epochs) :\n",
    "    np.random.seed(0)\n",
    "    return train_model(x_train_norm.T, y_train_mat.T, [28*28, 50, 25, 10], activations,\n",
    "                            initializations,\n",
    "                            epochs = epochs, learning_rate = 0.005, show_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-00fa822fa5c7>:108: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dA_last = -(np.divide(y_batch, A_last) - np.divide(1 - y_batch, 1 - A_last))/x_batch.shape[1]\n",
      "<ipython-input-2-00fa822fa5c7>:108: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dA_last = -(np.divide(y_batch, A_last) - np.divide(1 - y_batch, 1 - A_last))/x_batch.shape[1]\n",
      "<ipython-input-2-00fa822fa5c7>:59: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ = dA * act_derivates[activation](Z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: nan\n"
     ]
    }
   ],
   "source": [
    "params, costs = test_initializations(['uniform', 'uniform', 'uniform'], epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boum. Même pas un début de quelque chose, le système est déjà mort. Passer par des distributions uniformes, c'est bof. On va tester la division par une constante (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: 3.4451083524077366\n",
      "Epoch #2: 3.2095969256610655\n",
      "Epoch #3: 2.9372929160907137\n",
      "Epoch #4: 2.722872978659204\n",
      "Epoch #5: 2.2754168865291873\n",
      "Epoch #6: 1.8077371861216853\n",
      "Epoch #7: 1.4202047548039582\n",
      "Epoch #8: 1.2050923686359423\n",
      "Epoch #9: 0.9899931903889271\n",
      "Epoch #10: 0.7827429621616919\n",
      "Epoch #11: 0.6483106040722668\n",
      "Epoch #12: 0.5664563399518844\n",
      "Epoch #13: 0.5102771884965723\n",
      "Epoch #14: 0.4679848803323316\n",
      "Epoch #15: 0.4345764114964007\n",
      "Epoch #16: 0.4070456281168636\n",
      "Epoch #17: 0.3836346298939539\n",
      "Epoch #18: 0.36350435259368297\n",
      "Epoch #19: 0.34586114861486666\n",
      "Epoch #20: 0.3302383516982213\n"
     ]
    }
   ],
   "source": [
    "params, costs = test_initializations(['uniform_100', 'uniform_100', 'uniform_100'], epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set : 95.281667%\n",
      "Accuracy on test set : 94.610000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-00fa822fa5c7>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "def accuracy(x, y, params, layer_activations):\n",
    "    results = np.argmax(model_forward_pass(x, layer_activations, params)['A'+str(len(layer_activations))], axis = 0)\n",
    "    return np.mean(results == y)\n",
    "\n",
    "print('Accuracy on training set : %f%%' % (100*accuracy(x_train_norm.T, y_train.T, params, activations)))\n",
    "print('Accuracy on test set : %f%%'     % (100*accuracy(x_test_norm.T , y_test.T , params, activations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est plutôt très bon, et avec 20 itérations seulement.\n",
    "\n",
    "On teste maintenant une simple distribution gaussienne (sans facteur multiplicateur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-00fa822fa5c7>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n",
      "<ipython-input-2-00fa822fa5c7>:108: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dA_last = -(np.divide(y_batch, A_last) - np.divide(1 - y_batch, 1 - A_last))/x_batch.shape[1]\n",
      "<ipython-input-2-00fa822fa5c7>:108: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dA_last = -(np.divide(y_batch, A_last) - np.divide(1 - y_batch, 1 - A_last))/x_batch.shape[1]\n",
      "<ipython-input-2-00fa822fa5c7>:59: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ = dA * act_derivates[activation](Z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: nan\n"
     ]
    }
   ],
   "source": [
    "params, costs = test_initializations(['gaussian', 'gaussian', 'gaussian'], epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareil que la distribution uniforme, ça ne tient pas le premier round !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1.0125611633216307\n",
      "Epoch #2: 0.6442987536397736\n",
      "Epoch #3: 0.5307315466860846\n",
      "Epoch #4: 0.469224625860632\n",
      "Epoch #5: 0.4274914057945546\n",
      "Epoch #6: 0.39618900427008896\n",
      "Epoch #7: 0.37098109568591586\n",
      "Epoch #8: 0.35007864626282337\n",
      "Epoch #9: 0.3322431526405294\n",
      "Epoch #10: 0.31664505587135205\n",
      "Epoch #11: 0.3028466467498225\n",
      "Epoch #12: 0.2904549703974487\n",
      "Epoch #13: 0.2791373956663525\n",
      "Epoch #14: 0.2687646284683848\n",
      "Epoch #15: 0.2593010920991946\n",
      "Epoch #16: 0.25051651454632606\n",
      "Epoch #17: 0.2423501186660231\n",
      "Epoch #18: 0.2347128040883857\n",
      "Epoch #19: 0.22753375477933183\n",
      "Epoch #20: 0.22078085498835542\n",
      "Accuracy on training set : 96.906667%\n",
      "Accuracy on test set : 96.040000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-00fa822fa5c7>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "params, costs = test_initializations(['xavier', 'xavier', 'xavier'], epochs = 20)\n",
    "print('Accuracy on training set : %f%%' % (100*accuracy(x_train_norm.T, y_train.T, params, activations)))\n",
    "print('Accuracy on test set : %f%%'     % (100*accuracy(x_test_norm.T , y_test.T , params, activations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là c'est très intéressant : on voit que dès la première étape le cout est 3 fois moindre que tout à l'heure, on a une convergence beaucoup plus rapide !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: 0.8579472765944866\n",
      "Epoch #2: 0.6065799506694717\n",
      "Epoch #3: 0.5118657046569804\n",
      "Epoch #4: 0.4561725435260324\n",
      "Epoch #5: 0.41769774044723335\n",
      "Epoch #6: 0.3883302562777395\n",
      "Epoch #7: 0.36475675226187526\n",
      "Epoch #8: 0.34520759268055756\n",
      "Epoch #9: 0.3286283269816935\n",
      "Epoch #10: 0.31406060454078205\n",
      "Epoch #11: 0.30110103335478156\n",
      "Epoch #12: 0.2894717821576476\n",
      "Epoch #13: 0.27892852008999425\n",
      "Epoch #14: 0.26919729023451905\n",
      "Epoch #15: 0.26018325633595996\n",
      "Epoch #16: 0.2518317360532692\n",
      "Epoch #17: 0.2440085357983924\n",
      "Epoch #18: 0.236706242807445\n",
      "Epoch #19: 0.2299390923875491\n",
      "Epoch #20: 0.22347550008250824\n",
      "Accuracy on training set : 96.903333%\n",
      "Accuracy on test set : 95.790000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-00fa822fa5c7>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "params, costs = test_initializations(['he', 'he', 'he'], epochs = 20)\n",
    "print('Accuracy on training set : %f%%' % (100*accuracy(x_train_norm.T, y_train.T, params, activations)))\n",
    "print('Accuracy on test set : %f%%'     % (100*accuracy(x_test_norm.T , y_test.T , params, activations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1.2023023886461857\n",
      "Epoch #2: 0.702118236400899\n",
      "Epoch #3: 0.5636476647508493\n",
      "Epoch #4: 0.4943163394964271\n",
      "Epoch #5: 0.44897452009644767\n",
      "Epoch #6: 0.4156369027593031\n",
      "Epoch #7: 0.3890658982109316\n",
      "Epoch #8: 0.3669035000293687\n",
      "Epoch #9: 0.34813315060624606\n",
      "Epoch #10: 0.33179756328079835\n",
      "Epoch #11: 0.3172723735420891\n",
      "Epoch #12: 0.30425873103889717\n",
      "Epoch #13: 0.29243153115213366\n",
      "Epoch #14: 0.28171989983741247\n",
      "Epoch #15: 0.271788224535598\n",
      "Epoch #16: 0.2626436164822237\n",
      "Epoch #17: 0.25410711693936927\n",
      "Epoch #18: 0.24605809468439763\n",
      "Epoch #19: 0.23850388108851514\n",
      "Epoch #20: 0.23135582360542828\n",
      "Accuracy on training set : 96.770000%\n",
      "Accuracy on test set : 95.960000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-00fa822fa5c7>:17: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x) : return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "params, costs = test_initializations(['bengio', 'bengio', 'bengio'], epochs = 20)\n",
    "print('Accuracy on training set : %f%%' % (100*accuracy(x_train_norm.T, y_train.T, params, activations)))\n",
    "print('Accuracy on test set : %f%%'     % (100*accuracy(x_test_norm.T , y_test.T , params, activations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà. On constate que suivant les méthodes d'initialisation, on converge plus ou moins vite, les premières étapes sont plus ou moins optiomales, etc...\n",
    "\n",
    "Pour savoir laquelle est la meilleure... ça dépendra beaucoup du sujet. Faut tester :) Là c'est la méthode Xavier qui passe bien, mais en pratique su run réseau de taille \"utile\" on trouvera en général que He ou Bengio fonctionne mieux.\n",
    "\n",
    "Mais en tous les cas, on a quand même plus de 95% de succès à chaque fois avec 20 itérations uniquement !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
